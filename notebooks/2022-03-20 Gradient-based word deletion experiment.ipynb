{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dd8d98-0748-478b-8c82-6b77cfa44e53",
   "metadata": {},
   "source": [
    "# Gradient-based word deletion\n",
    "\n",
    "I trained a model to \"reidentify\" individuals from information about them. Specifically, this model tries to read the beginning of a Wikipedia page and predict (given the infoboxes of many people's Wikipedia page) which person the page is about. Now I'm going to try and fool this \"reidentifier\" model, and see how many words I have to delete in order to fool the reidentifier a certain percentage of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a8aa4-1973-48e9-842f-dc95dd66c3d4",
   "metadata": {},
   "source": [
    "## 1. Load the model and make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d252cb9-4ee5-42f2-98df-af6e72555090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a87966-0c59-45f7-8502-5e7da6710a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DocumentProfileMatchingTransformer with learning_rate = 2e-05\n"
     ]
    }
   ],
   "source": [
    "from model import DocumentProfileMatchingTransformer\n",
    "\n",
    "model = DocumentProfileMatchingTransformer(\n",
    "    dataset_name='wiki_bio',\n",
    "    model_name_or_path='distilbert-base-uncased',\n",
    "    num_workers=1,\n",
    "    loss_fn='exact',\n",
    "    num_neighbors=2048,\n",
    "    base_folder=\"/home/jxm3/research/deidentification/unsupervised-deidentification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29132408-7f12-472e-896b-494b99fda702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WikipediaDataModule with num_workers = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-5535f82839d9fec4.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-5b1c3941089b7f1b.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-8a9b289bc8e70b72.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-66c4158707dc80d7.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-cc7cc053a17e8b52.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-c66da49eff2c404b.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-d441ff9288a15e08.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba22652c5fae46a1b47e65b9f5a8c577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataloader import WikipediaDataModule\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "dm = WikipediaDataModule(\n",
    "    model_name_or_path='distilbert-base-uncased',\n",
    "    dataset_name='wiki_bio',\n",
    "    num_workers=min(8, num_cpus),\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=64,\n",
    "    max_seq_length=64,\n",
    "    redaction_strategy=\"\",\n",
    "    base_folder=\"/home/jxm3/research/deidentification/unsupervised-deidentification\",\n",
    ")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf32511-1971-482b-88b3-6848e6c83f5f",
   "metadata": {},
   "source": [
    "## 2. Define attack in TextAttack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996b9503-11dd-4378-bc97-794e195448be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9137c22-612e-44c4-8557-42dcdabd8fd5",
   "metadata": {},
   "source": [
    "### (a) Greedy word search + replace with `[MASK]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d76a3d-863f-4edb-94a1-c5a748644a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_method = textattack.search_methods.GreedyWordSwapWIR()\n",
    "\n",
    "class WordSwapSingleWord(textattack.transformations.word_swap.WordSwap):\n",
    "    \"\"\"Takes a sentence and transforms it by replacing with a single fixed word.\n",
    "    \"\"\"\n",
    "    single_word: str\n",
    "    def __init__(self, single_word: str = \"?\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.single_word = single_word\n",
    "\n",
    "    def _get_replacement_words(self, _word: str):\n",
    "        return [self.single_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee1838b-420d-4fc0-a849-45afcf3f1b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<AttackedText \"[MASK] my name is Jack\">,\n",
       " <AttackedText \"Hello [MASK] name is Jack\">,\n",
       " <AttackedText \"Hello my [MASK] is Jack\">,\n",
       " <AttackedText \"Hello my name [MASK] Jack\">,\n",
       " <AttackedText \"Hello my name is [MASK]\">]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation = WordSwapSingleWord(single_word='[MASK]')\n",
    "transformation(textattack.shared.AttackedText(\"Hello my name is Jack\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1de22-75f2-4049-8cf8-6023ddf4403d",
   "metadata": {},
   "source": [
    "### (b) \"Attack success\" as fullfilment of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d259f78c-2ece-4727-87e6-b7cb714e5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "class ChangeClassificationToBelowTopKClasses(textattack.goal_functions.ClassificationGoalFunction):\n",
    "    k: int\n",
    "    def __init__(self, *args, k: int = 1, **kwargs):\n",
    "        self.k = k\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _is_goal_complete(self, model_output, _):\n",
    "        original_class_score = model_output[self.ground_truth_output]\n",
    "        num_better_classes = (model_output > original_class_score).sum()\n",
    "        return num_better_classes >= self.k\n",
    "\n",
    "    def _get_score(self, model_output, _):\n",
    "        return 1 - model_output[self.ground_truth_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4405c-876a-4246-b33d-dbdfc80cd85c",
   "metadata": {},
   "source": [
    "## (c) Model wrapper that computes similarities of input documents with validation profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd35ad75-a681-4168-87b8-efee22ea41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "class MyModelWrapper(textattack.models.wrappers.ModelWrapper):\n",
    "    model: DocumentProfileMatchingTransformer\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    profile_embeddings: torch.Tensor\n",
    "    \n",
    "    def __init__(self, model: DocumentProfileMatchingTransformer, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.profile_embeddings = torch.tensor(model.val_embeddings)\n",
    "                 \n",
    "    def to(self, device):\n",
    "        self.model.to(device)\n",
    "        self.profile_embeddings.to(device)\n",
    "        return self # so semantics `model = MyModelWrapper().to('cuda')` works properly\n",
    "\n",
    "    def __call__(self, text_input_list, batch_size=32):\n",
    "        model_device = next(self.model.parameters()).device\n",
    "        tokenized_ids = self.tokenizer(text_input_list)\n",
    "        tokenized_ids = {k: torch.tensor(v).to(model_device) for k,v in tokenized_ids.items()}\n",
    "        \n",
    "        # TODO: implement batch size if we start running out of memory here.\n",
    "        with torch.no_grad():\n",
    "            document_embeddings = self.model.document_model(**tokenized_ids)\n",
    "            document_embeddings = document_embeddings['last_hidden_state'][:, 0, :] # (batch, document_emb_dim)\n",
    "            document_embeddings = self.model.lower_dim_embed(document_embeddings) # (batch, emb_dim)\n",
    "\n",
    "        document_to_profile_probs = torch.nn.functional.softmax(\n",
    "            document_embeddings @ self.profile_embeddings.T.to(model_device), dim=-1)\n",
    "        assert document_to_profile_probs.shape == (len(text_input_list), len(self.profile_embeddings))\n",
    "        return document_to_profile_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeee781-ae57-47c4-8679-52014830cc7e",
   "metadata": {},
   "source": [
    "## (d) Dataset that loads Wikipedia documents with names as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f61d2f9-9896-49d7-8b5b-501f05926784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_input_ids': tensor([[  101,  4831,  2745,  ..., 10722, 26896,   102],\n",
       "         [  101, 17504, 12022,  ...,     0,     0,     0],\n",
       "         [  101,  7929,  2319,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  9033,  2860,  ..., 12681,  5283,   102],\n",
       "         [  101,  7332, 27319,  ...,   102,     0,     0],\n",
       "         [  101,  3958, 11463,  ...,     0,     0,     0]]),\n",
       " 'document_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'document_redact_ner_input_ids': tensor([[  101,  4831,   103,  ...,  1010,  3140,   102],\n",
       "         [  101, 17504, 12022,  ...,     0,     0,     0],\n",
       "         [  101,   103,   103,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  9033,  2860,  ..., 15810,  8840,   102],\n",
       "         [  101,   103,   103,  ...,   102,     0,     0],\n",
       "         [  101,   103,   103,  ...,     0,     0,     0]]),\n",
       " 'document_redact_ner_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'document_redact_lexical_input_ids': tensor([[  101,   103,   103,  ...,  2000,  5271,   102],\n",
       "         [  101,   103,   103,  ...,     0,     0,     0],\n",
       "         [  101,   103,   103,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   103,  1036,  ...,  2232, 27969,   102],\n",
       "         [  101,   103, 27319,  ...,  1999,  4361,   102],\n",
       "         [  101,   103,   103,  ...,     0,     0,     0]]),\n",
       " 'document_redact_lexical_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'profile_input_ids': tensor([[  101,  6332,  1064,  ...,  4372,  2705,   102],\n",
       "         [  101,  2440, 18442,  ...,     0,     0,     0],\n",
       "         [  101,  2440, 18442,  ...,  1011,  2541,   102],\n",
       "         ...,\n",
       "         [  101,  6139,  1064,  ...,  2012, 11463,   102],\n",
       "         [  101,  2476,  1035,  ...,  4182,  1035,   102],\n",
       "         [  101,  2171,  1064,  ..., 12083,  1064,   102]]),\n",
       " 'profile_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'text_key_id': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b35ca719-fdad-45ee-b979-9c2a67d58c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import datasets\n",
    "\n",
    "class WikiDataset(textattack.datasets.Dataset):\n",
    "    dataset: datasets.Dataset\n",
    "    \n",
    "    def __init__(self, dm: WikipediaDataModule):\n",
    "        self.shuffled = True\n",
    "        self.dataset = dm.val_dataset\n",
    "        self.label_names = list(dm.val_dataset['name'])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[OrderedDict, int]:\n",
    "        input_dict = OrderedDict([\n",
    "            ('document', self.dataset['document'][i])\n",
    "        ])\n",
    "        return input_dict, self.dataset['text_key_id'][i].item()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abde44f-912e-4f31-8abd-8706ce99bbc6",
   "metadata": {},
   "source": [
    "## 3. Run attack once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3758747-400e-4261-bfc9-2c6714a81fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: No entry found for goal function <class '__main__.ChangeClassificationToBelowTopKClasses'>.\n",
      "textattack: Unknown if model of class <class 'model.DocumentProfileMatchingTransformer'> compatible with goal function <class '__main__.ChangeClassificationToBelowTopKClasses'>.\n"
     ]
    }
   ],
   "source": [
    "from textattack import Attack\n",
    "from textattack.constraints.pre_transformation import RepeatModification\n",
    "\n",
    "model_wrapper = MyModelWrapper(model, dm.tokenizer)\n",
    "model_wrapper.to('cuda')\n",
    "\n",
    "goal_function = ChangeClassificationToBelowTopKClasses(model_wrapper, k=10)\n",
    "constraints = [RepeatModification()]\n",
    "transformation = WordSwapSingleWord(single_word='[MASK]')\n",
    "search_method = textattack.search_methods.GreedyWordSwapWIR()\n",
    "\n",
    "attack = Attack(\n",
    "    goal_function, constraints, transformation, search_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3cbe2f6-609c-41be-9dbc-d33217451223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  unk\n",
      "  )\n",
      "  (goal_function):  ChangeClassificationToBelowTopKClasses\n",
      "  (transformation):  WordSwapSingleWord\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:  25%|██▌       | 1/4 [00:00<00:00, 12.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:  50%|█████     | 2/4 [00:00<00:00, 15.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 2 / 2:  50%|█████     | 2/4 [00:00<00:00, 15.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 3 / 3:  75%|███████▌  | 3/4 [00:00<00:00, 16.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Cody chupp (0%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .\n",
      "in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .\n",
      "this building was at one time believed to have later become the site of the cairo geniza .\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Chris evans (0%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "hui jun is a male former table tennis player from china .\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Anahita hemmati (0%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "okan Öztürk -lrb- born 30 november 1977 -rrb- is a turkish professional footballer .\n",
      "he currently plays as a striker for yeni malatyaspor .\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 4 / 4: 100%|██████████| 4/4 [00:00<00:00, 17.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Samuel spokes (0%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "marie stephan , -lrb- born march 14 , 1996 -rrb- is a professional squash player who represents france .\n",
      "she reached a career-high world ranking of world no. 101 in july 2015 .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+-------+\n",
      "| Attack Results                |       |\n",
      "+-------------------------------+-------+\n",
      "| Number of successful attacks: | 0     |\n",
      "| Number of failed attacks:     | 0     |\n",
      "| Number of skipped attacks:    | 4     |\n",
      "| Original accuracy:            | 0.0%  |\n",
      "| Accuracy under attack:        | 0.0%  |\n",
      "| Attack success rate:          | 0%    |\n",
      "| Average perturbed word %:     | nan%  |\n",
      "| Average num. words per input: | 34.75 |\n",
      "| Avg num queries:              | nan   |\n",
      "+-------------------------------+-------+"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/textattack/metrics/attack_metrics/words_perturbed.py:83: RuntimeWarning: Mean of empty slice.\n",
      "  average_perc_words_perturbed = self.perturbed_word_percentages.mean()\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/textattack/metrics/attack_metrics/attack_queries.py:39: RuntimeWarning: Mean of empty slice.\n",
      "  avg_num_queries = self.num_queries.mean()\n",
      "textattack: Logging to CSV at path results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "result: Cody chupp (0%) --> [SKIPPED]\n",
      "\n",
      "pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .\n",
      "in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .\n",
      "this building was at one time believed to have later become the site of the cairo geniza .\n",
      "\n",
      "result: Chris evans (0%) --> [SKIPPED]\n",
      "\n",
      "hui jun is a male former table tennis player from china .\n",
      "\n",
      "result: Anahita hemmati (0%) --> [SKIPPED]\n",
      "\n",
      "okan Öztürk -lrb- born 30 november 1977 -rrb- is a turkish professional footballer .\n",
      "he currently plays as a striker for yeni malatyaspor .\n",
      "\n",
      "result: Samuel spokes (0%) --> [SKIPPED]\n",
      "\n",
      "marie stephan , -lrb- born march 14 , 1996 -rrb- is a professional squash player who represents france .\n",
      "she reached a career-high world ranking of world no. 101 in july 2015 .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/textattack/loggers/csv_logger.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.df = self.df.append(row, ignore_index=True)\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/textattack/loggers/csv_logger.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.df = self.df.append(row, ignore_index=True)\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/textattack/loggers/csv_logger.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.df = self.df.append(row, ignore_index=True)\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/textattack/loggers/csv_logger.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.df = self.df.append(row, ignore_index=True)\n",
      "<ipython-input-52-33a57859b686>:21: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>original_score</th>\n",
       "      <th>perturbed_score</th>\n",
       "      <th>original_output</th>\n",
       "      <th>perturbed_output</th>\n",
       "      <th>ground_truth_output</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>result_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .<SPLIT>in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .<SPLIT>this building was at one time believed to have later become the site of the cairo geniza .<SPLIT></td>\n",
       "      <td>pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .<SPLIT>in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .<SPLIT>this building was at one time believed to have later become the site of the cairo geniza .<SPLIT></td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>7243</td>\n",
       "      <td>7243</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hui jun is a male former table tennis player from china .<SPLIT></td>\n",
       "      <td>hui jun is a male former table tennis player from china .<SPLIT></td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>2378</td>\n",
       "      <td>2378</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>okan Öztürk -lrb- born 30 november 1977 -rrb- is a turkish professional footballer .<SPLIT>he currently plays as a striker for yeni malatyaspor .<SPLIT></td>\n",
       "      <td>okan Öztürk -lrb- born 30 november 1977 -rrb- is a turkish professional footballer .<SPLIT>he currently plays as a striker for yeni malatyaspor .<SPLIT></td>\n",
       "      <td>0.999952</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>755</td>\n",
       "      <td>755</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marie stephan , -lrb- born march 14 , 1996 -rrb- is a professional squash player who represents france .<SPLIT>she reached a career-high world ranking of world no. 101 in july 2015 .<SPLIT></td>\n",
       "      <td>marie stephan , -lrb- born march 14 , 1996 -rrb- is a professional squash player who represents france .<SPLIT>she reached a career-high world ranking of world no. 101 in july 2015 .<SPLIT></td>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>10043</td>\n",
       "      <td>10043</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm # tqdm provides us a nice progress bar.\n",
    "from textattack.loggers import CSVLogger # tracks a dataframe for us.\n",
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "from textattack import Attacker\n",
    "from textattack import AttackArgs\n",
    "\n",
    "\n",
    "attack_args = AttackArgs(num_examples=4)\n",
    "dataset = WikiDataset(dm)\n",
    "\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "\n",
    "results_iterable = attacker.attack_dataset()\n",
    "\n",
    "logger = CSVLogger(color_method='html')\n",
    "\n",
    "for result in results_iterable:\n",
    "    print('result:', result)\n",
    "    logger.log_attack_result(result)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(logger.df.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dc05f-8fca-42b9-a524-7c31f54d3e92",
   "metadata": {},
   "source": [
    "## 4. Run attack in loop and make plot for multiple values of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93071875-f139-4841-8c47-cc14a327f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (4) code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7baf912-9611-4dd2-b6fd-f8ca71f4097e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
